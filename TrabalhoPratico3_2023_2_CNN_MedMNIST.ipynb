{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsansao/idl/blob/main/TrabalhoPratico3_2023_2_CNN_MedMNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trabalho Prático 3 - 2023-2\n",
        "## Redes convolucionais, bases do MedMNIST\n",
        "\n",
        "Neste trabalho vamos explorar as bases do [MedMNIST](https://medmnist.com/).\n",
        "\n",
        "A MedNIST v2 é um dataset de larga escala inspirado no MNIST. É uma coleção de dados biomédicos padronizados em 2D e 3D. As imagens tem dimensões 28x28 (monocromáticas ou RGB).\n",
        "\n",
        "Para esse trabalho, vamos optar por bases de imagens 2D para efetuar classificação binária ou multi-classe.\n",
        "\n",
        "O site do MedNIST mostra maiores detalhes sobre as bases.\n",
        "\n",
        "A idéia é que você escolha uma das bases e efetue a tarefa de treinar as redes nas seguintes condições:\n",
        "\n",
        "1. Rede convolucional simples\n",
        "2. Rede convolucional complexa (multiplas camadas e mais recursos adicionais)\n",
        "3. Treinamento com AutoKeras\n",
        "\n",
        "A idéia é obter o melhor modelo para cada um dos items. Para isso, considere a melhor acurácia de validação de cada modelo treinado.\n",
        "\n",
        "Para escolher a base, utilize a seguinte referência, baseada no último dígito da sua matrícula.\n",
        "\n",
        "1. 'pathmnist',\n",
        "2. 'dermamnist',\n",
        "3. 'octmnist',\n",
        "4. 'pneumoniamnist',\n",
        "5. 'bloodmnist',\n",
        "6. 'tissuemnist',\n",
        "7. 'organamnist',\n",
        "8. 'organcmnist'\n",
        "9. 'organsmnist'\n",
        "\n",
        "ou se for 0. 'breastmnist',\n",
        "\n"
      ],
      "metadata": {
        "id": "6X_QMdI33glC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instalação do MedMNIST\n",
        "\n"
      ],
      "metadata": {
        "id": "qX26KJ4iMZx0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install medmnist"
      ],
      "metadata": {
        "id": "BQT5wfEviJva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Escolha o dataset do MedMNIST\n",
        "# @markdown Escolha no menu abaixo:\n",
        "data_flag = \"breastmnist\"  # @param ['pathmnist', 'dermamnist', 'octmnist', 'pneumoniamnist', 'breastmnist', 'bloodmnist', 'tissuemnist', 'organamnist', 'organcmnist', 'organsmnist']\n",
        "# @markdown ---"
      ],
      "metadata": {
        "id": "03cCq9t3d_VH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelo CNN simples"
      ],
      "metadata": {
        "id": "T-9it1wsM-2n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importação do dataset"
      ],
      "metadata": {
        "id": "GXb3hR-hMqx6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxI6uPLwiB2C"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "import medmnist\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from medmnist import INFO, Evaluator\n",
        "from medmnist.info import DEFAULT_ROOT\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_root = DEFAULT_ROOT\n",
        "output_root = './images'\n",
        "\n",
        "\n",
        "info = INFO[data_flag]\n",
        "task = info['task']\n",
        "_ = getattr(medmnist, INFO[data_flag]['python_class'])(split=\"train\", root=input_root, download=True)\n",
        "\n",
        "n_classes = len(info['label'])\n",
        "n_channels = info['n_channels']\n",
        "\n",
        "output_root = os.path.join(output_root, data_flag, time.strftime(\"%y%m%d_%H%M%S\"))\n",
        "\n",
        "if not os.path.isdir(output_root):\n",
        "  os.makedirs(output_root)\n",
        "\n",
        "npz_file = np.load(os.path.join(input_root, \"{}.npz\".format(data_flag)))\n",
        "\n",
        "x_train = npz_file['train_images']\n",
        "y_train = npz_file['train_labels']\n",
        "x_val = npz_file['val_images']\n",
        "y_val = npz_file['val_labels']\n",
        "x_test = npz_file['test_images']\n",
        "y_test = npz_file['test_labels']\n",
        "\n",
        "\n",
        "#main(data_flag, num_trials, input_root, output_root, gpu_ids, run, model_path)"
      ],
      "metadata": {
        "id": "pBcgIFiAihvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Augmentation"
      ],
      "metadata": {
        "id": "fJNZKdm5M0mQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "metadata": {
        "id": "UIYc7gZJvYbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rank = x_train.ndim\n",
        "\n",
        "if rank < 4:\n",
        "  x_train = x_train[..., np.newaxis]\n",
        "  x_test = x_test[..., np.newaxis]\n",
        "  x_val = x_val[..., np.newaxis]\n",
        "\n"
      ],
      "metadata": {
        "id": "0exPvcfwZFUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datagen_train = ImageDataGenerator(\n",
        "    rescale=1.0/255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "datagen_test = ImageDataGenerator(rescale=1.0/255)\n",
        "# compute quantities required for featurewise normalization\n",
        "# (std, mean, and principal components if ZCA whitening is applied)\n",
        "datagen_train.fit(x_train)\n",
        "datagen_test.fit(x_test)\n"
      ],
      "metadata": {
        "id": "X-qfRnkEtsqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Criação das categorias"
      ],
      "metadata": {
        "id": "7qVLC0pMM5Be"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "info['label']"
      ],
      "metadata": {
        "id": "cCWtz00Gwmgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "y_train = to_categorical(y_train,n_classes)\n",
        "y_test = to_categorical(y_test,n_classes)"
      ],
      "metadata": {
        "id": "t5AomhjnzmvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, n_channels)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(n_classes, activation='softmax')\n",
        "\n",
        "])"
      ],
      "metadata": {
        "id": "7PLSC3Jqwc0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "rE9sPYmFxJMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(datagen_train.flow(x_train, y_train),\n",
        "         validation_data=datagen_test.flow(x_test, y_test),\n",
        "         epochs=10)\n"
      ],
      "metadata": {
        "id": "eGRE6KpZwEED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelo CNN com mais recursos"
      ],
      "metadata": {
        "id": "vPZtDLnLNNc6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.datasets import cifar10\n",
        "from keras import regularizers, optimizers\n",
        "import numpy as np\n",
        "from matplotlib import pyplot"
      ],
      "metadata": {
        "id": "nrKmewMviC7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "import medmnist\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from medmnist import INFO, Evaluator\n",
        "from medmnist.info import DEFAULT_ROOT\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "na0DDD-ciRfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_root = DEFAULT_ROOT\n",
        "output_root = './images'\n",
        "\n",
        "\n",
        "info = INFO[data_flag]\n",
        "task = info['task']\n",
        "_ = getattr(medmnist, INFO[data_flag]['python_class'])(split=\"train\", root=input_root, download=True)\n",
        "\n",
        "n_classes = len(info['label'])\n",
        "n_channels = info['n_channels']\n",
        "\n",
        "\n",
        "output_root = os.path.join(output_root, data_flag, time.strftime(\"%y%m%d_%H%M%S\"))\n",
        "\n",
        "if not os.path.isdir(output_root):\n",
        "  os.makedirs(output_root)\n",
        "\n",
        "npz_file = np.load(os.path.join(input_root, \"{}.npz\".format(data_flag)))\n",
        "\n",
        "x_train = npz_file['train_images']\n",
        "y_train = npz_file['train_labels']\n",
        "x_val = npz_file['val_images']\n",
        "y_val = npz_file['val_labels']\n",
        "x_test = npz_file['test_images']\n",
        "y_test = npz_file['test_labels']\n",
        "\n",
        "\n",
        "y_train = to_categorical(y_train,n_classes)\n",
        "y_test = to_categorical(y_test,n_classes)\n",
        "#main(data_flag, num_trials, input_root, output_root, gpu_ids, run, model_path)"
      ],
      "metadata": {
        "id": "DMwVf4anidmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rank = x_train.ndim\n",
        "\n",
        "if rank < 4:\n",
        "  x_train = x_train[..., np.newaxis]\n",
        "  x_test = x_test[..., np.newaxis]\n",
        "  x_val = x_val[..., np.newaxis]"
      ],
      "metadata": {
        "id": "dZMwaFlVa78u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datagen_train = ImageDataGenerator(\n",
        "    rescale=1.0/255,\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=False)\n",
        "\n",
        "\n",
        "datagen_test = ImageDataGenerator(rescale=1.0/255,\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=False)\n",
        "\n",
        "# compute quantities required for featurewise normalization\n",
        "# (std, mean, and principal components if ZCA whitening is applied)\n",
        "datagen_train.fit(x_train)\n",
        "datagen_test.fit(x_test)\n"
      ],
      "metadata": {
        "id": "i-jxPvmSjIfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build the model\n",
        "\n",
        "# number of hidden units variable\n",
        "# we are declaring this variable here and use it in our CONV layers to make it easier to update from one place\n",
        "base_hidden_units = 32\n",
        "\n",
        "# l2 regularization hyperparameter\n",
        "weight_decay = 1e-4\n",
        "\n",
        "# instantiate an empty sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# CONV1\n",
        "# notice that we defined the input_shape here because this is the first CONV layer.\n",
        "# we don’t need to do that for the remaining layers\n",
        "model.add(Conv2D(base_hidden_units, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# CONV2\n",
        "model.add(Conv2D(base_hidden_units, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# CONV3\n",
        "model.add(Conv2D(2*base_hidden_units, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# CONV4\n",
        "model.add(Conv2D(2*base_hidden_units, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "# CONV5\n",
        "model.add(Conv2D(4*base_hidden_units, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# CONV6\n",
        "model.add(Conv2D(4*base_hidden_units, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "# FC7\n",
        "model.add(Flatten())\n",
        "model.add(Dense(n_classes, activation='softmax'))\n",
        "\n",
        "# print model summary\n",
        "model.summary()\n",
        "\n",
        "opt = tf.keras.optimizers.RMSprop(learning_rate=0.0003)\n",
        "\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "batch_size = 64\n",
        "epochs=125\n",
        "\n"
      ],
      "metadata": {
        "id": "Jsi9os7Vh2i8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_filepath = '/tmp/checkpoint'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)"
      ],
      "metadata": {
        "id": "SpBubav0505T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(datagen_train.flow(x_train, y_train,batch_size=batch_size),\n",
        "         validation_data=datagen_test.flow(x_test, y_test),\n",
        "         steps_per_epoch=x_train.shape[0] / batch_size,\n",
        "         epochs=epochs, callbacks=[model_checkpoint_callback])\n"
      ],
      "metadata": {
        "id": "xnAeu-_CjUKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploração com Autokeras"
      ],
      "metadata": {
        "id": "Nr0smT56NFfe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install autokeras\n",
        "import autokeras as ak"
      ],
      "metadata": {
        "id": "-G6fq4PT92hj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "input_root = DEFAULT_ROOT\n",
        "output_root = './images'\n",
        "\n",
        "\n",
        "info = INFO[data_flag]\n",
        "task = info['task']\n",
        "_ = getattr(medmnist, INFO[data_flag]['python_class'])(split=\"train\", root=input_root, download=True)\n",
        "\n",
        "n_classes = len(info['label'])\n",
        "n_channels = info['n_channels']\n",
        "\n",
        "output_root = os.path.join(output_root, data_flag, time.strftime(\"%y%m%d_%H%M%S\"))\n",
        "\n",
        "if not os.path.isdir(output_root):\n",
        "  os.makedirs(output_root)\n",
        "\n",
        "npz_file = np.load(os.path.join(input_root, \"{}.npz\".format(data_flag)))\n",
        "\n",
        "x_train = npz_file['train_images']\n",
        "y_train = npz_file['train_labels']\n",
        "x_val = npz_file['val_images']\n",
        "y_val = npz_file['val_labels']\n",
        "x_test = npz_file['test_images']\n",
        "y_test = npz_file['test_labels']"
      ],
      "metadata": {
        "id": "iSDiDazwc3xk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the image classifier.\n",
        "clf = ak.ImageClassifier(overwrite=True, max_trials=1)\n",
        "clf.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    # Use your own validation set.\n",
        "    validation_data=(x_test, y_test),\n",
        "    epochs=5,\n",
        ")"
      ],
      "metadata": {
        "id": "f1RkZ06Z9YtY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}